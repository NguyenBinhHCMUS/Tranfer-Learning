{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cho_meo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMyxWYd8eya4yxVcaXseEdv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kMUW_78a2nb6"},"source":["# Storage googleapis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2K5eYy5k2uSH","executionInfo":{"status":"ok","timestamp":1608641743395,"user_tz":-420,"elapsed":350328,"user":{"displayName":"Lol Nguyen","photoUrl":"","userId":"04820846355462089015"}},"outputId":"fde68652-c807-4ccc-db01-9b890128f724"},"source":["import os\r\n","\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import tensorflow_hub as hub\r\n","import matplotlib.pyplot as plt\r\n","import tensorflow_datasets as tfds\r\n","from PIL import Image\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, Reshape\r\n","from tensorflow.keras.layers.experimental.preprocessing import Resizing\r\n","\r\n","!wget -q https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\r\n","!unzip -q cats_and_dogs_filtered.zip\r\n","\r\n","TRAIN_DIR = \"./cats_and_dogs_filtered/train\"\r\n","VAL_DIR = \"./cats_and_dogs_filtered/validation\"\r\n","\r\n","IMG_SIZE = (224, 224)\r\n","INP_SHAPE = (*IMG_SIZE, 3)\r\n","BATCH_SIZE = 64\r\n","\r\n","def gen_new_data(data_folder, augmented=True):\r\n","  if augmented:\r\n","    data_gen = ImageDataGenerator(rescale=1./255, rotation_range=40,\r\n","                                  width_shift_range=0.2, height_shift_range=0.2,\r\n","                                  horizontal_flip=True)\r\n","  else:\r\n","    data_gen = ImageDataGenerator(rescale=1./255)\r\n","\r\n","  data = data_gen.flow_from_directory(\r\n","      data_folder,\r\n","      target_size=IMG_SIZE,\r\n","      batch_size=BATCH_SIZE,\r\n","      class_mode=\"binary\",\r\n","  )\r\n","  return data\r\n","\r\n","ds_train = gen_new_data(TRAIN_DIR, augmented=True)\r\n","ds_val = gen_new_data(VAL_DIR, augmented=False)\r\n","\r\n","mobilenet_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\" \r\n","\r\n","mobilenet = Sequential()\r\n","mobilenet.add(Resizing(224, 224, input_shape=INP_SHAPE))\r\n","mobilenet.add(hub.KerasLayer(mobilenet_url, trainable=False))\r\n","mobilenet.add(Dense(1, activation=\"sigmoid\"))\r\n","\r\n","mobilenet.compile(loss='binary_crossentropy',\r\n","              optimizer='adam',\r\n","              metrics=['accuracy'])\r\n","\r\n","mobilenet.fit(ds_train, validation_data=ds_val,\r\n","              batch_size=32, epochs=10,verbose=1)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Epoch 1/10\n","32/32 [==============================] - 36s 858ms/step - loss: 0.4047 - accuracy: 0.8225 - val_loss: 0.1143 - val_accuracy: 0.9740\n","Epoch 2/10\n","32/32 [==============================] - 27s 846ms/step - loss: 0.1495 - accuracy: 0.9465 - val_loss: 0.0799 - val_accuracy: 0.9810\n","Epoch 3/10\n","32/32 [==============================] - 27s 844ms/step - loss: 0.1075 - accuracy: 0.9656 - val_loss: 0.0653 - val_accuracy: 0.9830\n","Epoch 4/10\n","32/32 [==============================] - 27s 844ms/step - loss: 0.0951 - accuracy: 0.9690 - val_loss: 0.0600 - val_accuracy: 0.9830\n","Epoch 5/10\n","32/32 [==============================] - 27s 843ms/step - loss: 0.0862 - accuracy: 0.9714 - val_loss: 0.0555 - val_accuracy: 0.9830\n","Epoch 6/10\n","32/32 [==============================] - 27s 845ms/step - loss: 0.0935 - accuracy: 0.9655 - val_loss: 0.0530 - val_accuracy: 0.9840\n","Epoch 7/10\n","32/32 [==============================] - 27s 838ms/step - loss: 0.0792 - accuracy: 0.9717 - val_loss: 0.0523 - val_accuracy: 0.9840\n","Epoch 8/10\n","32/32 [==============================] - 27s 845ms/step - loss: 0.0721 - accuracy: 0.9739 - val_loss: 0.0528 - val_accuracy: 0.9850\n","Epoch 9/10\n","32/32 [==============================] - 27s 842ms/step - loss: 0.0870 - accuracy: 0.9677 - val_loss: 0.0458 - val_accuracy: 0.9840\n","Epoch 10/10\n","32/32 [==============================] - 27s 842ms/step - loss: 0.0700 - accuracy: 0.9735 - val_loss: 0.0450 - val_accuracy: 0.9850\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fd99eb2b080>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Vr_KgNUu4tb9"},"source":["# DATASETS"]},{"cell_type":"code","metadata":{"id":"jXh7dVFTopxc","executionInfo":{"status":"ok","timestamp":1608643685191,"user_tz":-420,"elapsed":1559,"user":{"displayName":"Lol Nguyen","photoUrl":"","userId":"04820846355462089015"}}},"source":["from sklearn.preprocessing import minmax_scale"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"lG7AYlsM4vGU","executionInfo":{"status":"ok","timestamp":1608641821197,"user_tz":-420,"elapsed":1039,"user":{"displayName":"Lol Nguyen","photoUrl":"","userId":"04820846355462089015"}}},"source":["ds_train, ds_info = tfds.load(\r\n","    'cats_vs_dogs',\r\n","    split=tfds.Split.TRAIN,\r\n","    with_info=True,\r\n",")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPzXPje1vAFy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4ou3b8b8GRT","executionInfo":{"status":"ok","timestamp":1608648573882,"user_tz":-420,"elapsed":1186,"user":{"displayName":"Lol Nguyen","photoUrl":"","userId":"04820846355462089015"}}},"source":["num_class = 2\r\n","normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset = -1)\r\n","\r\n","def preprocess(features):\r\n","  \r\n","    image = tf.image.resize(features['image'], [224, 224])\r\n","    image = tf.cast(image, tf.float32)\r\n","\r\n","    #label = tf.one_hot(features['label'], depth=num_class)\r\n","    label = features['label']\r\n","    return image, label\r\n","\r\n","def solution_model():\r\n","\r\n","  train_dataset = ds_train.map(preprocess).batch(32)\r\n","  train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\r\n","\r\n","  mobilenet_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\" \r\n","  model = Sequential()\r\n","  \r\n","  model.add(hub.KerasLayer(mobilenet_url, trainable=False, input_shape=(224, 224, 3)))\r\n","  model.add(Dense(1, activation=\"sigmoid\"))\r\n","\r\n","  model.compile(loss='binary_crossentropy',\r\n","              optimizer='adam',\r\n","              metrics=['accuracy'])\r\n","\r\n","  model.fit(train_dataset,\r\n","              batch_size=32, epochs=10,verbose=1)\r\n","  \r\n","  return model\r\n"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YduvY4_8JE1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608648975132,"user_tz":-420,"elapsed":400110,"user":{"displayName":"Lol Nguyen","photoUrl":"","userId":"04820846355462089015"}},"outputId":"a5694c57-084d-450b-cc29-c5015eaa7cf5"},"source":["model = solution_model()"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","727/727 [==============================] - 36s 45ms/step - loss: 0.2643 - accuracy: 0.8806\n","Epoch 2/10\n","727/727 [==============================] - 33s 46ms/step - loss: 0.1281 - accuracy: 0.9488\n","Epoch 3/10\n","727/727 [==============================] - 34s 46ms/step - loss: 0.1193 - accuracy: 0.9528\n","Epoch 4/10\n","727/727 [==============================] - 34s 46ms/step - loss: 0.1138 - accuracy: 0.9544\n","Epoch 5/10\n","727/727 [==============================] - 33s 46ms/step - loss: 0.1097 - accuracy: 0.9558\n","Epoch 6/10\n","727/727 [==============================] - 34s 46ms/step - loss: 0.1065 - accuracy: 0.9569\n","Epoch 7/10\n","727/727 [==============================] - 33s 45ms/step - loss: 0.1038 - accuracy: 0.9580\n","Epoch 8/10\n","727/727 [==============================] - 33s 46ms/step - loss: 0.1016 - accuracy: 0.9591\n","Epoch 9/10\n","727/727 [==============================] - 34s 47ms/step - loss: 0.0997 - accuracy: 0.9596\n","Epoch 10/10\n","727/727 [==============================] - 34s 46ms/step - loss: 0.0981 - accuracy: 0.9606\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UonvW2VNmsP6"},"source":[""],"execution_count":null,"outputs":[]}]}